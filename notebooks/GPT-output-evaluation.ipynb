{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2778ce",
   "metadata": {},
   "source": [
    "# LLMs Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba947f",
   "metadata": {},
   "source": [
    "#### This notebook presents a methodology to evaluate the text generated by a LLM comparing it to a manual labelling created by human annotators through three metrics: BERTScore, ROUGE, and BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8441172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc592b2",
   "metadata": {},
   "source": [
    "### Import the dataframe containing the generated text and the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32f563d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>label</th>\n",
       "      <th>extracted_needs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hey fellow travel enthusiasts! ðŸ‘‹    So here's ...</td>\n",
       "      <td>suggestions upon best christmas activities and...</td>\n",
       "      <td>Christmas-themed places recommendations, Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello! We will be taking a family trip (teens ...</td>\n",
       "      <td>accomodation advices</td>\n",
       "      <td>Hotel selection advice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Where could I deposit luggage nearby Penn Stat...</td>\n",
       "      <td>luggage storage options</td>\n",
       "      <td>Luggage storage options, Reliable mobile appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi i have posted a few times before and really...</td>\n",
       "      <td>itinerary planning advices, shopping advices, ...</td>\n",
       "      <td>Itinerary planning advices, Shopping advices, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi looking for advice on where to stay on a bu...</td>\n",
       "      <td>accomodation advices, travelling advices from ...</td>\n",
       "      <td>Affordable accomadations, Safe accomodations f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               TEXT   \n",
       "0           0  Hey fellow travel enthusiasts! ðŸ‘‹    So here's ...  \\\n",
       "1           1  Hello! We will be taking a family trip (teens ...   \n",
       "2           2  Where could I deposit luggage nearby Penn Stat...   \n",
       "3           3  Hi i have posted a few times before and really...   \n",
       "4           4  Hi looking for advice on where to stay on a bu...   \n",
       "\n",
       "                                               label   \n",
       "0  suggestions upon best christmas activities and...  \\\n",
       "1                               accomodation advices   \n",
       "2                            luggage storage options   \n",
       "3  itinerary planning advices, shopping advices, ...   \n",
       "4  accomodation advices, travelling advices from ...   \n",
       "\n",
       "                                     extracted_needs  \n",
       "0  Christmas-themed places recommendations, Trans...  \n",
       "1                             Hotel selection advice  \n",
       "2  Luggage storage options, Reliable mobile appli...  \n",
       "3  Itinerary planning advices, Shopping advices, ...  \n",
       "4  Affordable accomadations, Safe accomodations f...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"..\\data\\gpt4_output.xlsx\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e8df9",
   "metadata": {},
   "source": [
    "<h2>Statistical Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be93815",
   "metadata": {},
   "source": [
    "<p>In this work, we are interested in counting how many customer needs were extracted by GPT-4, to compare its output with the labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e880d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_needs(df_col):\n",
    "    count = 0\n",
    "    for i in df_col:\n",
    "        if type(i) != float:\n",
    "            count = count + i.count(\",\") + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd98ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of needs extracted by GPT-4:  769\n",
      "Number of needs identified by human annotators:  561\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of needs extracted by GPT-4: \", count_needs(df[\"extracted_needs\"]))\n",
    "print(\"Number of needs identified by human annotators: \", count_needs(df[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46074c",
   "metadata": {},
   "source": [
    "<h2>Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2afc15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label example:  suggestions upon best christmas activities and must sees, transportation advice, budget planning, restaurant recommendations\n",
      "Extracted needs example:  Christmas-themed places recommendations, Transportation advices, Budget planning advices, Airport recommendations\n"
     ]
    }
   ],
   "source": [
    "label_example = df[\"label\"][0]\n",
    "print(\"Label example: \", label_example)\n",
    "extracted_example = df[\"extracted_needs\"][0]\n",
    "print(\"Extracted needs example: \", extracted_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb76bf7",
   "metadata": {},
   "source": [
    "### BERTScore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07d15ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def bertscore(label, predicted):\n",
    "    P, R, F1 = score(predicted, label,model_type=\"microsoft/deberta-xlarge-mnli\",rescale_with_baseline=False, lang=\"en\", verbose=False)\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "382d9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7698975205421448\n",
      "Recall:  0.7301522493362427\n",
      "F1:  0.7494983673095703\n"
     ]
    }
   ],
   "source": [
    "Prec, Rec, F_1 = bertscore([label_example], [extracted_example])\n",
    "print(\"Precision: \", Prec.item())\n",
    "print(\"Recall: \", Rec.item())\n",
    "print(\"F1: \", F_1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62366e20",
   "metadata": {},
   "source": [
    "### ROUGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ddb863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def rougescore(label, predicted):\n",
    "    scores = scorer.score(label, predicted)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d473b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (Rouge-1):  0.4799999999999999\n",
      "F1 (Rouge-L):  0.4799999999999999\n"
     ]
    }
   ],
   "source": [
    "P1, R1, F1, PL, RL, FL = rougescore(label_example, extracted_example)\n",
    "print(\"F1 (Rouge-1): \", F1)\n",
    "print(\"F1 (Rouge-L): \", FL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81182e",
   "metadata": {},
   "source": [
    "### BLEU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91782642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    # Tokenize the strings into lists of words\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d21d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24450989066362577\n"
     ]
    }
   ],
   "source": [
    "print(calculate_bleu_score(label_example, extracted_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7d675",
   "metadata": {},
   "source": [
    "### Evaluate the LLM's output all together with all the metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be9f2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(df, col_label, col_eval):\n",
    "    # take the number of rows of the dataframe\n",
    "    n = len(df.index)\n",
    "    # BERTScore can be calculated directly on the whole column\n",
    "    Prec, Rec, F_1 = bertscore(df[col_eval].tolist(), df[col_label].tolist())\n",
    "    # For Rouge and BLEU we need to iterate all the rows of the dataframe\n",
    "    # initialize the variables to store the sum of the metrics, to be used to calculate the average\n",
    "    f1_rouge1 = 0\n",
    "    f1_rougeL = 0\n",
    "    bleu_t = 0\n",
    "    # iterate over all the rows \n",
    "    for index, row in df.iterrows():\n",
    "        # calculate the metrics for the current row\n",
    "        P1, R1, F1, PL, RL, FL = rougescore(row[col_label], row[col_eval])\n",
    "        bleu = calculate_bleu_score(row[col_label], row[col_eval])\n",
    "        # sum the calculated metrics to the variables defined before\n",
    "        bleu_t += bleu\n",
    "        f1_rouge1 += F1\n",
    "        f1_rougeL += FL\n",
    "    # calculate the mean of the metrics\n",
    "    bleu_t = round(bleu_t/n, 3)\n",
    "    f1_rouge1 = round(f1_rouge1/n, 3)\n",
    "    f1_rougeL = round(f1_rougeL/n, 3)\n",
    "    return round(Prec.mean().item(), 3), round(Rec.mean().item(), 3), round(F_1.mean().item(), 3), f1_rouge1, f1_rougeL, bleu_t\n",
    "\n",
    "# define a function to clean the text from the symbols\n",
    "def prepare(col1):\n",
    "    col = col1.str.replace(r',', lambda x: x.group().replace(',', ' '), regex=True)\n",
    "    col = col.str.replace(r',', lambda x: x.group().replace('-', ' '), regex=True)\n",
    "    col = col.str.lower()\n",
    "    return col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25b1031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the two texts\n",
    "df[\"label\"] = prepare(df[\"label\"])\n",
    "df[\"extracted_needs\"] = prepare(df[\"extracted_needs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24b51c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval NEEDS GPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (BERTScore):  0.795\n",
      "Recall (BERTScore):  0.759\n",
      "F1 (BERTScore):  0.774\n",
      "F1 (Rouge-1):  0.607\n",
      "F1 (Rouge-L):  0.588\n",
      "BLEU:  0.399\n"
     ]
    }
   ],
   "source": [
    "print(\"eval NEEDS GPT\")\n",
    "prec_bert, rec_bert, f1_bert, f1_rou1, f1_rouL, bleu = evaluation(df.head(10), \"label\", \"extracted_needs\")\n",
    "print(\"Precision (BERTScore): \", prec_bert)\n",
    "print(\"Recall (BERTScore): \", rec_bert)\n",
    "print(\"F1 (BERTScore): \", f1_bert)\n",
    "print(\"F1 (Rouge-1): \", f1_rou1)\n",
    "print(\"F1 (Rouge-L): \", f1_rouL)\n",
    "print(\"BLEU: \", bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b66ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de643e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986f64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a3866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
